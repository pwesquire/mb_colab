{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pwesquire/mb_colab/blob/main/Profanity_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import user's Google Drive\n",
        "# You will need /content/drive/MyDrive/formatted_bad_names.csv (https://drive.google.com/file/d/1pobWSRbL5ihp2fgd1hxH1sbyiFBgUZ3L/view?usp=sharing)\n",
        "# and /content/drive/MyDrive/international_names.csv (https://drive.google.com/file/d/1B-gziiuVsGewAPhSBWZO5gk1rCiwHisN/view?usp=sharing)\n",
        "# and /content/drive/MyDrive/labeled_names.csv (https://drive.google.com/file/d/1csnUvTnD4AQ-XHZT7SLVuagEnj08O9Wp/view?usp=sharing)\n",
        "# which Patrick can share with you.\n",
        "#\n",
        "# Click the drive icon in the top right and \"Add Shortcut to Drive\" in the root of your\n",
        "# Google Drive directory and this should all run\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fRwGHwk8r8a",
        "outputId": "1a96a0c8-43be-49bb-b10e-b935559b967c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLPRVfaow9MH"
      },
      "outputs": [],
      "source": [
        "#@title Run on TensorFlow 2.x\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "# !pip install better_profanity\n",
        "# from better_profanity import profanity as pf \n",
        "from pathlib import Path\n",
        "!pip install -q -U \"tensorflow-text==2.8.*\"\n",
        "import tensorflow_text as tft\n",
        "import tensorflow_hub as hub\n",
        "!pip install -q tf-models-official==2.7.0\n",
        "from official.nlp import optimization \n",
        "# import functools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set log level\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# Panda options for outputting tables\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "\n",
        "# Creating Panda DataFrame objects from our CSV files\n",
        "bad_names = pd.read_csv(\"/content/drive/MyDrive/formatted_bad_names.csv\")\n",
        "good_names = pd.read_csv(\"/content/drive/MyDrive/international_names.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix some missing data in our csv files\n",
        "# • Take random country codes from good_names and insert them into bad names list (due to missing country codes)\n",
        "# • Transform bad names to title case\n",
        "rand_good = good_names.sample(2224, ignore_index=True)\n",
        "bad_names.fillna(rand_good, inplace=True)\n",
        "bad_names_titlecase = bad_names.transform({\"first_name\": lambda x: x.title(), \"last_name\": lambda x: x.title(), \"gender\": lambda x: x, \"country_code\": lambda x: x})\n",
        "print(bad_names_titlecase)"
      ],
      "metadata": {
        "id": "LKZh7ox-y50g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply label to indicate the profanity score of each row of the DataFrames (data sets)\n",
        "good_names = good_names.assign(label=0.0)\n",
        "bad_names = bad_names_titlecase.assign(label=1.0)"
      ],
      "metadata": {
        "id": "T1woZ0hkObvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine good and bad names together\n",
        "all_names = good_names.append(bad_names, ignore_index=True)\n",
        "\n",
        "# Make sure there are no missing values\n",
        "all_names.fillna({\"first_name\": \"\", \"last_name\": \"\", \"country_code\": \"NA\"}) # Note - NA is Namibia, change this\n",
        "\n",
        "# Create full name field from first and last\n",
        "all_names['full_name'] = all_names[['first_name','last_name']].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
        "\n",
        "# Method to test profanity filter library - Deprecated\n",
        "# Could we do label_profanity_mb(first_name, last_name) and just mark anybody with \"poop\" as a name 1.0\n",
        "# TODO: Bergen will find bad words list\n",
        "# TODO: Patrick will build this in 5 min.\n",
        "# Then we can do (pseudocode, obvs):\n",
        "#   def label_profanity_for_dataFrame(incomingDataFrame) { for each df: label_profanity(df.first, df.last) }\n",
        "#   def label_profanity(first_name, last_name) { ... }\n",
        "#   ... then do ...\n",
        "#   label_profanity_in_some_basic_way()\n",
        "#   fancy=false\n",
        "#   if fancy:\n",
        "#     label_profanity_in_some_fancy_way()\n",
        "#   return_or_set_profanity_score()\n",
        "def label_profanity(name):\n",
        "  if pf.contains_profanity(name):\n",
        "    return 1.0\n",
        "  else:\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "\n",
        "# all_names['label'] = all_names[['full_name']].apply(lambda row: label_profanity(row['full_name']), axis=1)\n",
        "# ^^ was profanity library attempt but took too long -- was about 3 seconds per row^^\n",
        "# could maybe work for single submissions... \n",
        "# but would have to be a fleet of lambda's to handle 1000 submissions/hr\n",
        "\n",
        "# removing unused columns/fields from dataFrame\n",
        "all_names = all_names.drop([\"first_name\",\"last_name\",\"gender\"], axis=1)\n",
        "# randomizing the rows\n",
        "all_names_rand = all_names.reindex(np.random.permutation(all_names.index))\n",
        "\n",
        "# Save a file with names labelled with a profanity score\n",
        "# Possibly: in the future, could get rid of many previous steps and start with this labelled dataset\n",
        "#   Could break out earlier code into a method like prepare_and_label_dataset()\n",
        "filepath = Path('/content/drive/MyDrive/labeled_names.csv') \n",
        "all_names_rand.to_csv(filepath, index=False)  "
      ],
      "metadata": {
        "id": "6Oi-ZB5PIUHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking first X rows as dataset for training the model\n",
        "train_df = all_names_rand.head(40000)\n",
        "# Taking other rowns for validation and testing, respectively\n",
        "val_df = all_names_rand.iloc[45001:50000]\n",
        "test_df = all_names_rand.tail(5000)"
      ],
      "metadata": {
        "id": "5cWAp8VS7HWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that columns have the correct datatype\n",
        "# TODO: streamline by doing all_names.astype()\n",
        "train_df = train_df.astype({'full_name': 'string', 'country_code': 'string', 'label':'float32'})\n",
        "val_df = val_df.astype({'full_name': 'string', 'country_code': 'string', 'label':'float32'})\n",
        "test_df = test_df.astype({'full_name': 'string', 'country_code': 'string', 'label':'float32'})\n",
        "# print(train_df['full_name'].dtype)\n",
        "# print(all_names.tail(5))"
      ],
      "metadata": {
        "id": "L1MLC1iifThY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# From here on out it is a bit murky\n",
        "#\n",
        "\n",
        "# Remove column from dataFrame for use... so model doesn't see any label we've applied\n",
        "train_labels = train_df.pop('label')\n",
        "val_labels = val_df.pop('label')\n",
        "test_labels = test_df.pop('label')\n",
        "\n",
        "# tf.data.Dataset is one possible TensorFlow format that you can feed into a model\n",
        "# We are passing labels in... doing something with 'full_name & label'\n",
        "# Explanation from Patrick 9/26/22:\n",
        "#   A Dataset is a bunch of Tensors\n",
        "#   Each Tensor has a Shape and a DataType\n",
        "#   Tensors in a Dataset can be arranged in many different structures\n",
        "#   Gets confusing...\n",
        "#   Every type of Tensor has a full gigantic article in api_docs... (https://www.tensorflow.org/api_docs/python/tf)\n",
        "#     Can do with csv, raw data, so many ways\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((tf.strings.as_string(train_df['full_name']), train_labels))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((tf.strings.as_string(val_df['full_name']), val_labels))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((tf.strings.as_string(test_df['full_name']), test_labels))\n",
        "\n",
        "# Autotune is something to improve the performance through caching\n",
        "# These are just values from a tutorial\n",
        "# AUTOTUNE _is_ a configurable buffer size, in the end\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "o1JqMmNMNGjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grabbing some predefined models (like a gem/library) from the TensorFlow Hub\n",
        "# Usage:\n",
        "#    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
        "#    preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "#    encoder_inputs = preprocessor(text_input)\n",
        "# Explanation from PTorrez\n",
        "#   Layer is a function that is one step in a model\n",
        "#   Input layer is where your data enters a model\n",
        "#   Data often has some processing before it goes into the model\n",
        "#   Model is a sequence of layers\n",
        "#   Nodes are weighted that affect how subsequent layers receive their inputs\n",
        "#   A challenge is: How to process your data into a usable form to go into a model?\n",
        "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'"
      ],
      "metadata": {
        "id": "tdSMLB8mjT1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a model with a bunch of layers in it\n",
        "# Weighted nodes that define how (the data gets processed)\n",
        "# Output of each layer gets fed into the next layer\n",
        "# Sequence of layers:\n",
        "#   input: receives string inputs\n",
        "#   preprocessing: get vocabulary that is in the strings\n",
        "#   encoder: encode as tokens and maps them to integers (or floats?)\n",
        "#   net: puts those into a net with nodes\n",
        "#     Dropout - deletes some of the input, maybe to improve performance (unclear - from tutorial)\n",
        "#     Dense - convert into a float between zero and one* (*probably not true)\n",
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(None,), dtype=tf.string, name='sentences')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "metadata": {
        "id": "aun6RUGsnGPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up model\n",
        "classifier_model = build_classifier_model()\n",
        "\n",
        "# Configure how many rounds of weighting adjustments it goes through\n",
        "epochs = 5\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "# Learning Rate (a very tiny number)\n",
        "# How much the weighting can (affect the data/change) each epoch(?)\n",
        "init_lr = 3e-5\n",
        "\n",
        "# Create optimizer based on this configuration\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ],
      "metadata": {
        "id": "bXPNLGZpvYdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss - not sure \n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "# Metrics - not sure\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "# Compile model to create a sequence to feed data through\n",
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "\n",
        "# Fit the model - calculate the weights to make the model be able to predict labels for data\n",
        "# history is a variable... we don't use it but maybe for analysis for how well it's performing. Not sure.\n",
        "# After calling model.fit you can call model.predict or model.evaluate. Not sure the difference.\n",
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(x=train_ds,\n",
        "                               validation_data=val_ds,\n",
        "                               epochs=epochs)"
      ],
      "metadata": {
        "id": "9XqjPYB4vijN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate is an analysis tool to see how well model performs on a test dataset\n",
        "# Gauge how well the model performs\n",
        "loss, accuracy = classifier_model.evaluate(test_ds)\n",
        "\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Predict\n",
        "# A good test of the model would be something like:\n",
        "#    keras_model.predict(aSingleRowsWorthOfData) to see the label that it would predict\n"
      ],
      "metadata": {
        "id": "1MxfjdUWvu7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Everything below here is from previous attempts ###\n",
        "\n",
        "Next steps:\n",
        "\n",
        "* Set up a very basic Profanity Filter API that doesn't even try to use an ML model\n",
        "  * Patrick builds a Ruby \"Profanity Filter\" API on SENDY that, given a request {first,last,email, region,country,postal_code} will always return { score : .5 }\n",
        "  * Bergen gets Patrick a dataset of bad words\n",
        "  * Patrick updates the Profanity Filter API to, given a single request: \n",
        "    * If EITHER first or last name match a bad word, we give it a .9 and return that value to the caller. If not, we return a .1\n",
        "* Set up a Python-based Profanity Detection Model API\n",
        "  * First build it to return an error message and no value  { error : { message : \"No profanity score returned\" }  }\n",
        "  * Then, we build it out to return a value and work on improving the model"
      ],
      "metadata": {
        "id": "tBV0s-JOQ1GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# train_labels = tf.constant(train_df['label'])\n",
        "\n",
        "# Preprocess the input strings.\n",
        "train_data = preprocess_data(train_ds)\n",
        "val_data = preprocess_data(val_ds)\n",
        "test_data = preprocess_data(test_ds)\n",
        "\n",
        "# Build the Keras model.\n",
        "keras_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=[None], dtype=tf.int64),\n",
        "\n",
        "    # Feature layer goes here?\n",
        "    tf.keras.layers.Dense(32),\n",
        "    tf.keras.layers.Activation(tf.nn.relu),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "keras_model.compile(loss='binary_crossentropy', optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "keras_model.fit(train_data, train_labels, epochs=5,callbacks=[tensorboard_callback])\n",
        "# print(keras_model.predict(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "bzltAHrVTkaS",
        "outputId": "65326214-1c6a-4fa5-cb55-bb712e1254cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-402cef28efee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m ])\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/core/dense.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mlast_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlast_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m       raise ValueError('The last dimension of the inputs to a Dense layer '\n\u001b[0m\u001b[1;32m    140\u001b[0m                        \u001b[0;34m'should be defined. Found None. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                        f'Full input shape received: {input_shape}')\n",
            "\u001b[0;31mValueError\u001b[0m: The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (None, None)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_bad = pd.DataFrame(data={'full_name': \"BJ CUMMINGS\", 'country_code': 'CA'}, index=[0]).astype({'full_name': 'string', 'country_code': 'string'})\n",
        "test_good = pd.DataFrame(data={'full_name': \"Bjork Gonzo\", 'country_code': 'JP'}, index=[0]).astype({'full_name': 'string', 'country_code': 'string'})\n",
        "processed_bad = preprocess_data(test_bad)\n",
        "processed_good = preprocess_data(test_good)\n",
        "\n",
        "test_prediction_bad = keras_model.predict(processed_bad)\n",
        "test_prediction_good = keras_model.predict(processed_good)\n",
        "\n",
        "print(\"bad name prediction:\",test_prediction_bad)\n",
        "\n",
        "print(\"good name prediction:\",test_prediction_good)"
      ],
      "metadata": {
        "id": "RCYto2cU6I1L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8577724-fbdb-40b6-a447-1e3777570936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bad name prediction: [[-0.01348143]]\n",
            "good name prediction: [[-0.01752897]]\n"
          ]
        }
      ]
    }
  ]
}